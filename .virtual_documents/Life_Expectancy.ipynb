# Lib import
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import charset_normalizer


with open("train.csv", 'rb') as rawdata:
    result = charset_normalizer.detect(rawdata.read(10000))
print(result)


df = pd.read_csv("train.csv", encoding = "utf-8")
df.head()



with open("test.csv", 'rb') as rawdata:
    result = charset_normalizer.detect(rawdata.read(10000))
print(result)
test_df = pd.read_csv("test.csv", encoding='utf-8')
test_df.describe()


df.describe()






# Revisar valores nulos
print("Valores nulos en Train Dataset:\n", df.isnull().sum())

# Identificar duplicados
print("\nFilas duplicadas en Train Dataset:", df.duplicated().sum())

# Tipos de datos
print("\nTipos de datos del Train Dataset:\n", df.dtypes)

# Explorar valores únicos de las columnas categóricas
categorical_columns = df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nValores únicos en {col}:\n", df[col].unique())



# Revisar valores nulos
print("Valores nulos en Train Dataset:\n", test_df.isnull().sum())

# Identificar duplicados
print("\nFilas duplicadas en Train Dataset:", test_df.duplicated().sum())

# Tipos de datos
print("\nTipos de datos del Train Dataset:\n", test_df.dtypes)

# Explorar valores únicos de las columnas categóricas
categorical_columns = test_df.select_dtypes(include=['object']).columns
for col in categorical_columns:
    print(f"\nValores únicos en {col}:\n", test_df[col].unique())







sns.histplot(data = df['Life expectancy '])
plt.title('Histogram of Life expectancy')
plt.show





df['Status'] = df['Status'].apply(lambda x: 1 if x == 'Developing' else 0)
df['Status']


test_df['Status'] = test_df['Status'].apply(lambda x: 1 if x == 'Developing' else 0)
test_df['Status']





status_count = df['Status'].value_counts()
sns.barplot(data = status_count)
plt.show()











from sklearn.feature_selection import SelectKBest, f_classif

df_num = df.select_dtypes(include=['int', 'float'])
selector = SelectKBest(score_func=f_classif, k=10)
selector.fit_transform(df_num, df['Life expectancy '])
selected_columns = df_num.columns[selector.get_support()]

print("Selected Features Shape:", selected_columns.shape)


selected_columns_test = selected_columns.drop('Life expectancy ')
test_selected = test_df[selected_columns_test]
print("Selected Features Shape:", test_selected.shape)





df_corr = df_num[selected_columns].corr()

plt.figure(figsize=(10, 8))
sns.heatmap(df_corr,annot=True, cmap='coolwarm')
plt.show()





from sklearn.model_selection import train_test_split

df_train = df_num[selected_columns]

X = df_train.drop('Life expectancy ', axis = 1)
y = df_train['Life expectancy ']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)






from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

# Entrena el modelo
model_lr = LinearRegression()
model_lr.fit(X_train, y_train)

# Realiza predicciones
y_pred_lr = model_lr.predict(X_test)

# Calcula el error
mse_lr = mean_squared_error(y_test, y_pred_lr)
print(f'MSE de Regresión Lineal: {mse_lr}')
r2_lr = r2_score(y_test, y_pred_lr)
print(f'R² de Regresión Lineal: {r2_lr}')


from sklearn.tree import DecisionTreeRegressor

# Entrena el modelo
model_tree = DecisionTreeRegressor(random_state=42)
model_tree.fit(X_train, y_train)

# Realiza predicciones
y_pred_tree = model_tree.predict(X_test)

# Calcula el error
mse_tree = mean_squared_error(y_test, y_pred_tree)
print(f'MSE del Árbol de Decisión: {mse_tree}')
r2_tree = r2_score(y_test, y_pred_tree)
print(f'R² del Árbol de Decisión: {r2_tree}')


from sklearn.ensemble import RandomForestRegressor

# Entrena el modelo
model_rf = RandomForestRegressor(random_state=42)
model_rf.fit(X_train, y_train)

# Realiza predicciones
y_pred_rf = model_rf.predict(X_test)

# Calcula el error
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'MSE de Random Forest: {mse_rf}')
r2_rf = r2_score(y_test, y_pred_rf)
print(f'R² de Random Forest: {r2_rf}')


from xgboost import XGBRegressor

# Entrena el modelo
model_xgb = XGBRegressor(random_state=42)
model_xgb.fit(X_train, y_train)

# Realiza predicciones
y_pred_xgb = model_xgb.predict(X_test)

# Calcula el error
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'MSE de XGBoost: {mse_xgb}')
r2_xgb = r2_score(y_test, y_pred_xgb)
print(f'R² de XGBoost: {r2_xgb}')


from sklearn.svm import SVR

# Entrena el modelo
model_svr = SVR()
model_svr.fit(X_train, y_train)

# Realiza predicciones
y_pred_svr = model_svr.predict(X_test)

# Calcula el error
mse_svr = mean_squared_error(y_test, y_pred_svr)
print(f'MSE de SVR: {mse_svr}')
r2_svr = r2_score(y_test, y_pred_svr)
print(f'R² de SVR: {r2_svr}')





from sklearn.model_selection import cross_val_score

model = LinearRegression()

scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

mean_score = np.mean(scores)
std_dev_score = np.std(scores)

print(f'Media del R²: {mean_score}')
print(f'Desviación estándar del R²: {std_dev_score}')


from sklearn.model_selection import cross_val_score

model = DecisionTreeRegressor(random_state=42)

scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

mean_score = np.mean(scores)
std_dev_score = np.std(scores)

print(f'Media del R²: {mean_score}')
print(f'Desviación estándar del R²: {std_dev_score}')


##### from sklearn.model_selection import cross_val_score

model = RandomForestRegressor(random_state=42)

scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

mean_score = np.mean(scores)
std_dev_score = np.std(scores)

print(f'Media del R²: {mean_score}')
print(f'Desviación estándar del R²: {std_dev_score}')


from sklearn.model_selection import cross_val_score

model = XGBRegressor(random_state=42)

scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2')

mean_score = np.mean(scores)
std_dev_score = np.std(scores)

print(f'Media del R²: {mean_score}')
print(f'Desviación estándar del R²: {std_dev_score}')





from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
# Define el modelo
model = Ridge()

# Define la cuadrícula de parámetros a buscar
param_grid = {
  'alpha': [0.01, 0.1, 1, 10, 100],
  'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg']
}
# Configura el objeto GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='r2', verbose=2)

# Ajusta el modelo a los datos de entrenamiento
grid_search.fit(X_train, y_train)

# Obtén los mejores parámetros y la mejor puntuación de R²
best_parameters = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Mejores parámetros encontrados: {best_parameters}")
print(f"Mejor puntuación R² en validación cruzada: {best_score}")
# Usa el mejor estimador para hacer predicciones
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calcula R² en el conjunto de prueba
from sklearn.metrics import r2_score
r2_best_model = r2_score(y_test, y_pred)
print(f'R² en el conjunto de prueba: {r2_best_model}')





from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Define el modelo
model = RandomForestRegressor(random_state=42)

# Define la cuadrícula de parámetros a buscar
param_grid = {
  'n_estimators': [50,100, 200, 300],
  'max_depth': [None, 10, 20, 30],
  'min_samples_split': [2, 5, 10],
  'min_samples_leaf': [1, 2, 4]
}

# Configura el objeto GridSearchCV
grid_search = GridSearchCV(estimator=model, param_grid=param_grid,
                           cv=5, n_jobs=-1, scoring='r2', verbose=2)

# Ajusta el modelo a los datos de entrenamiento
grid_search.fit(X_train, y_train)

# Obtén los mejores parámetros y la mejor puntuación de R²
best_parameters = grid_search.best_params_
best_score = grid_search.best_score_

print(f"Mejores parámetros encontrados: {best_parameters}")
print(f"Mejor puntuación R² en validación cruzada: {best_score}")

# Usa el mejor estimador para hacer predicciones
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

# Calcula R² en el conjunto de prueba
from sklearn.metrics import r2_score
r2_best_model = r2_score(y_test, y_pred)
print(f'R² en el conjunto de prueba: {r2_best_model}')






predictions = model.predict(test_selected)

output = pd.DataFrame({
    "Id": (test_df.index) + 1, 
    "Life Expectancy Prediction": predictions
})

output.to_csv("submission.csv", index=False)

print("Predicciones realizadas y guardadas en submission.csv")

